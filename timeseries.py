# -*- coding: utf-8 -*-
"""timeseries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sEFG1c52NLMoj3GUgDCF3nqe70qbcvgJ

# Import libraries and pakages
"""

import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

# Data handling
import pandas as pd
pd.set_option('display.max_columns', None)
import openpyxl
import numpy as np
from zipfile import ZipFile
import zipfile

# Vizualisation (Matplotlib, Plotly, Seaborn, etc. )
import seaborn as sns
sns.set_style('whitegrid')
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
import plotly.colors as colors
from plotly.subplots import make_subplots

# EDA (pandas-profiling, etc. )
import scipy.stats as stats
from statsmodels.tsa.stattools import kpss
from scipy.stats import t, ttest_ind, chi2_contingency


# Feature Processing (Scikit-learn processing, etc. )
from sklearn.preprocessing import StandardScaler, OneHotEncoder


# Other packages
import os, pickle

"""# Data Loading"""

# Read CSV and change date column from object to date type

holidays=pd.read_csv("holidays_events.csv", parse_dates =['date'])
oil=pd.read_csv("oil.csv", parse_dates =['date'])
sample=pd.read_csv("sample_submission.csv")
stores=pd.read_csv("stores.csv")
test=pd.read_csv("test.csv", parse_dates =['date'])
train=pd.read_csv("train.csv", parse_dates =['date'])
transactions=pd.read_csv("transactions.csv", parse_dates =['date'])

"""# Exploratory Data Analysis
Training data
"""

# view first 5 rows
train.head()

# view info
train.info()

"""Features and descriptions of train and test dataset



store_nbr	identifies the store at which the products are sold.

family identifies the type of product sold.

sales	gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).

onpromotion	gives the total number of items in a product family that were being promoted at a store at a given date.
"""

train.describe()

# Check for missing values using isna()
missing_values = train.isna().sum()
print(missing_values)

# Count missing values in each column
missing_counts = missing_values.sum()
print(missing_counts)

# Check if there are any missing values in the entire DataFrame
any_missing = missing_values.any().any()
print("\nAre there any missing values in the DataFrame?", any_missing)



# format numerical columns to 2 decimal places with comma separator
pd.options.display.float_format = '{:,.2f}'.format

# statistics for numerical columns
train.describe()

""" plotting frequencies of data point on yearly/monthly basis"""

# copy train dataset so that we work with a copy, not the original
train_copy = train.copy()

# extracting the year column
train_copy['year'] = pd.to_datetime(train['date']).dt.year

# Grouping by year and count the data points
year_data = train_copy.groupby('year').size().reset_index(name='counts')

# Plotting the data
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(year_data['year'], year_data['counts'], color='blue')
ax.set_xticks(year_data['year'])
ax.set_title('Count of Number of Data Points Each Year')
ax.set_xlabel('Year')
ax.set_ylabel('Year Counts')

plt.show()

"""There has been a uniformity of data counts from 2013-2016 but the count dropped in 2017"""

# extracting the month
train_copy['month'] = pd.to_datetime(train_copy['date']).dt.month

# Grouping by month and count the data points
monthly_data = train_copy.groupby('month').size().reset_index(name='counts')

# Renaming the month values for labeling
month_names = [
    'January', 'February', 'March', 'April', 'May', 'June',
    'July', 'August', 'September', 'October', 'November', 'December'
]
monthly_data['month'] = monthly_data['month'].apply(lambda x: month_names[x - 1])

# Plotting the data
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(monthly_data['month'], monthly_data['counts'], color='blue')
ax.set_title('Count of Number of Data Points Each Month')
ax.set_xlabel('Months')
ax.set_ylabel('Monthly Counts')

plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Ensure labels are not cut off
plt.show()

"""There is some uniformity in distribution of datapoints across the months

Total sales by store
"""

# Filter rows where unit_sales > 0
train_copy = train_copy[train['sales'] > 0]

# Group by 'store_nbr' and calculate the sum of 'unit_sales'
store_unit_sales = train_copy.groupby('store_nbr')['sales'].sum().reset_index()

# Set the figure size
sns.set(rc={'figure.figsize': (12, 6)})

# Create a barplot
sns.barplot(x='store_nbr', y='sales', data=store_unit_sales, palette='rainbow')

# Set titles and labels
plt.title('Total Sales per Store')
plt.xlabel('Stores')
plt.ylabel('Total Sales')

# Show the plot
plt.tight_layout()
plt.show()

"""We can say Store number 3,44,45,46,47,48,49, 51 have the highest volume of sales

Unit sales daily
"""

# Filter rows where unit_sales > 0
train_copy = train_copy[train_copy['sales'] > 0]

# Group by 'date' and calculate the sum of 'sales'
sales_date_agg = train_copy.groupby('date')['sales'].sum().reset_index()

# Create a lineplot using Plotly Express
fig = px.line(sales_date_agg, x='date', y='sales', title='Plot of Total Sales Over Time (Daily)')
fig.update_layout(width=1200, height=600)

# Add a range slider
fig.update_xaxes(
    rangeslider_visible=True,
    rangeselector=dict(
        buttons=list([
            dict(count=1, label="1m", step="month", stepmode="backward"),
            dict(count=6, label="6m", step="month", stepmode="backward"),
            dict(count=1, label="YTD", step="year", stepmode="todate"),
            dict(count=1, label="1y", step="year", stepmode="backward"),
            dict(step="all")
        ])
    )
)
# Show the plot
fig.show()

"""upward trend year on year

***Test data***

The test data, has the same features as the training data, except sales, which we will predict the target sales for the dates in this file.

The dates in the test data are for the 15 days after the last date in the training data.
"""

# view first 5 rows
test.head()

# view info
test.info()

# check for missing values

test.isna().sum()

test.describe()

"""***Transactions Dataset***

Contains date, store_nbr and transaction made on that specific date.
"""

# view first 5 rows
transactions.head()

# view info
transactions.info()

# check for missing values

transactions.isna().sum()

# Visualize transactions over the years with plotly

fig = px.line(transactions, y="transactions",x='date', title="Transaction data over time")

# enable range slider with range selector
fig.update_xaxes(
    rangeslider_visible=True,
    rangeselector=dict(
        buttons=list([
            dict(count=1, label="1m", step="month", stepmode="backward"),
            dict(count=6, label="6m", step="month", stepmode="backward"),
            dict(count=1, label="YTD", step="year", stepmode="todate"),
            dict(count=1, label="1y", step="year", stepmode="backward"),
            dict(step="all")
        ])
    )
)
fig.show()

"""Big spikes is transactions in december every year; indicates seasonality

***Stores Dataset***

Store metadata, including city, state, type, and cluster.

cluster is a grouping of similar stores.
"""

# view first 5 rows
stores.head()

# view info
stores.info()

# check for missing values

stores.isna().sum()

"""***Oil Dataset***

Daily oil price which includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and its economical health is highly vulnerable to shocks in oil prices.)
"""

# view first 5 rows
oil.head()

# view info
oil.info()

# check for missing values

oil.isna().sum()

# Oil prices over time

ax = oil.groupby(['date'], as_index=False)["dcoilwtico"].sum().plot(
    "date", "dcoilwtico", figsize=(12,6), title="Oil prices over time", color='blue')

plt.tick_params(axis='x', labelrotation=45)

"""***Holiday/Events Dataset***



*   Holidays and Events, with metadata



NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was celebrated, look for the corresponding row where type is Transfer.

For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.

*   Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).
"""

# view forst 5 rows
holidays.head()

# view info
holidays.info()

# check for missing values

holidays.isna().sum()

"""# Analytical Questions

**1. Is the train dataset complete (has all the required dates)?**
"""

# Check for missing values
if train.isnull().values.any():
  print("The dataset is not complete. There are missing values.")
else:
  print("The dataset is complete.")

"""The dataset has no missing value. However let's check the completion of the dates.

***Date completion***
"""

# Checking for completeness of the dates
# Min and max dates (range of dates)

train.date.min(), train.date.max()

# Actual recorded number of days in train dataset

train['date'].nunique()

# range between min and max dates

min_date = train['date'].min()
max_date = train['date'].max()
days_range = pd.date_range(start=min_date, end=max_date)
num_days = len(days_range)
num_days

expected_dates = pd.date_range(start=min_date, end=max_date)
expected_dates

# Missing dates

missing_dates = expected_dates.difference(train['date'].unique())
missing_dates

"""The train dataset is mising 4 dates.These missing dates follow a pattern (december 25th) from 2013-2016. This date is a worldwide holiday, and also a holiday in Ecuador.

The assumption is that no data was collected on this day every year as the shops may be closed down on christmas each year.
"""

missing_df = list(missing_dates)
print(pd.DataFrame(missing_df))

train.columns

# Add missing dates in a DataFrame

new_dates = pd.DataFrame(missing_df, columns = ['date'])
new_dates

"""let us add the missing dates to the train dataset"""

train = pd.concat([train, new_dates], ignore_index=False)
train

train.isna().sum()

"""we now have some missing values due to adding missing dates. we will deal with these later on.

**2. Which dates have the lowest and highest sales for each year?**
"""

# Highest sales date in each year

train_copy['year'] = train_copy['date'].dt.year
max_sales = train_copy.groupby('year')['sales'].idxmax()
Result_max = train.loc[max_sales]
Result_max

# Horizontal bar chart of dates with highest sales each year

sns.barplot(data=Result_max, y="date", x="sales", palette='crest')
plt.ylabel("Date")
plt.xlabel("Total sales")
plt.title("Highest sales date in each year")
plt.show()

"""2016(11/12) has the highest number of sales (124,717)"""

# lowest sales date in each year

min_sales = train_copy.groupby('year')['sales'].idxmin()
Result_min = train.loc[min_sales]
Result_min

# Horizontal bar chart of dates with lowest sales each year

sns.barplot(data=Result_min, y="date", x="sales", palette='crest')
plt.ylabel("Date")
plt.xlabel("Total sales")
plt.title("lowest sales date in each year")
plt.show()

"""2013(08/06) has the lowest number of sales (0.12)

**3. Did the earthquake impact sales?**

The 2016 Ecuador earthquake occurred on April 16 at 18:58:37 ECT with a moment magnitude of 7.8
"""

# Set the 'date' column as the index

train_copy.set_index('date', inplace=True)

# Resample to weekly frequency, aggregating with mean

sales_daily_mean = train_copy["sales"].resample('D').mean()
sales_weekly_mean = train_copy["sales"].resample('W').mean()

# Sales for April to May 2016.
start, end = '2016-04', '2016-05'

# Analyse before and after the earthquake

fig, ax = plt.subplots(figsize=(12,6))

color_daily = 'blue'
color_weekly = 'green'

ax.plot(sales_daily_mean.loc[start:end], marker='.', linestyle='-', linewidth=0.5, label='Daily', color=color_daily)
ax.plot(sales_weekly_mean.loc[start:end], marker='o', markersize=8, linestyle='-', label='Weekly Mean Resample', color=color_weekly)
ax.set_ylabel("Total sales")
ax.set_xlabel("Date")
ax.tick_params(axis='x', labelrotation=45)
ax.legend()
ax.set_title("April and May 2016 total sales")

"""In the week of the earthquake (from April 14th to April 21st), sales increased during this week, reaching a peak on the day of the earthquake (16th), but then experienced a sharp decline for nearly two weeks following the earthquake, returning to normalcy.

**4. Are certain groups of stores selling more products? (Cluster, city, state, type)**
"""

# Merge train and stores datasets

train_stores = pd.merge(train, stores)

train_stores.head()

"""**Store sales by cluster**"""

# Sum of sales by cluster

sales_clusters = train_stores.groupby("cluster", as_index=False)["sales"].sum()
sales_clusters

# vertical Bar chart of sales by cluster

ax = sns.barplot(data=sales_clusters, x = "cluster", y = "sales", palette='viridis')
plt.xlabel("Stores by cluster")
plt.ylabel("Total sales")
plt.title("Sales of different Stores by cluster")
plt.show()

"""cluster 14 has the highest sales. cluster 16 has the lowest sales.

**Store sales by city**
"""

# Sum of sales by city

sales_city = train_stores.groupby("city", as_index=False)["sales"].sum()
sales_city

# horizontal Bar chart of sales by city

sns.barplot(data=sales_city, y = "city", x = "sales", palette='viridis')
plt.ylabel("City of  of stores")
plt.xlabel("Total sales")
plt.title("Sales of different Stores by City")
plt.show()

"""Quito has the highest total sales, as it is the capital, and Puyo the lowest

**Store sales by state**
"""

# Sum of sales by state

sales_state = train_stores.groupby("state", as_index=False)["sales"].sum()
sales_state

# horizontal Bar chart of sales by state

sns.barplot(data=sales_state, y = "state", x = "sales", palette='viridis')
plt.ylabel("States of stores")
plt.xlabel("Total sales")
plt.title("Sales of different Stores by State")
plt.show()

"""Pichincha has the highest total sales
Pastaza has the lowest total sales

**Store sales by type**
"""

# Sum of sales by store type

sales_type = train_stores.groupby("type", as_index=False)["sales"].sum()
sales_type

# vertical Bar chart of sales by store type

sns.barplot(data=sales_type, x = "type", y = "sales", palette='viridis')
plt.xlabel("Stores by store type")
plt.ylabel("Total sales")
plt.title("Sales of Stores by type")
plt.show()

"""type A nd D have the highest total sales

**5. Are sales affected by promotions, oil prices and holidays?**

***Sales and promotions***
"""

# check unique values product family
train['family'].unique()

# Filter rows with promotions and sum sales for top 10 families
promotions = train_copy[train_copy["onpromotion"] != 0].groupby("family")["sales"].sum().sort_values(ascending=False).head(10)

# Filter rows without promotions and sum sales for top 10 families
no_promotions = train_copy[train_copy["onpromotion"] == 0].groupby("family")["sales"].sum().sort_values(ascending=False).head(10)

# Create stacked bar chart to visualize sales on and off promotions

ax = promotions.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')
no_promotions.plot(kind='bar', stacked=True, ax=ax, colormap='cividis')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Add legend
plt.legend(["Promotions", "No Promotions"])

# Label the axes and title
plt.ylabel("Sales")
plt.xlabel("Products")
plt.title("Sales of products with or without promotion (Top 10)")

plt.tight_layout()
plt.show()

"""promotions increases sales for most common purchased items in groceries and beverages category.

***Sales and oil prices***
"""

# merge train and oil dataset

merge = train_copy.merge(
    oil,
    how='left',
    on=['date'])

merge.reset_index()
merge.head()

# Create a figure with two subplots
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))

# Plot the average sales by year
merge.groupby(['year'])['sales'].sum().plot.bar(ax=ax1, title='Total Sales by Year')
ax1.set_xlabel('Year')
ax1.set_ylabel('Total Sales')

# Plot the oil data overtime
sns.lineplot(data=merge, x='date', y='dcoilwtico', ax=ax2)
ax2.set_title('Oil price overtime')
ax2.set_xlabel('Year')
ax2.set_ylabel('oil price')

plt.tight_layout()
plt.show()

# Determine the correlation between sales and oil prices

correlation = merge['sales'].corr(merge['dcoilwtico'])
print(correlation)

"""Very weak and almost negligible negative correlation between sales and oil prices over time
Other factors are likely more influential in determining sales than oil prices

***Sales and holidays***
"""

# unique values in holidays column

holidays['type'].value_counts()

holidays.info()

# merge train and holidays dataset

merge_2 = train_copy.merge(
    holidays,
    how='left',
    on=['date'])

merge_2.reset_index()
merge_2.head()

merge_2.info()

# Group the data by holiday type and calculate the average sales for each type
avg_sales_by_type = merge_2.groupby("type").agg({"sales": "mean"})

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(12, 6))

# Plot a bar chart of the average sales by holiday type
avg_sales_by_type.plot(kind="bar", y="sales", ax=ax, legend=False)

# Add labels to the bars
for index, value in enumerate(avg_sales_by_type["sales"]):
    plt.text(index, value, str(round(value, 2)), ha='center', va='bottom', fontsize=12)

# Add a title and labels
ax.set_title("Average Sales by Holiday Type", fontsize=16)
ax.set_xlabel("Holiday Type", fontsize=14)
ax.set_ylabel("Average Sales", fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""Additional holidays have the highest average sales. However, holiday type barely has an effect on sales

**6. What analysis can we get from the date and its extractable features?**
"""

# Function to extract features from date column

def getDateFeatures(df, date):
    df['date'] = pd.to_datetime(df[date])
    df['month'] = df['date'].dt.month
    df['Month'] = df['date'].dt.month_name()
    df['Day'] = df['date'].dt.day_name()
    df['day_of_month'] = df['date'].dt.day
    df['day_of_year'] = df['date'].dt.dayofyear
    df['week_of_year'] = df['date'].dt.isocalendar().week
    df['day_of_week'] = df['date'].dt.dayofweek
    df['year'] = df['date'].dt.year
    df["is_weekend"] = np.where(df['day_of_week'] > 4, 1, 0)
    df['quarter'] = df['date'].dt.quarter
    return df

# run our train data through our function

train_copy = train_copy.reset_index()
train_data = getDateFeatures(train_copy, "date")
train_data.head()

"""***Sales on Pay Day (Semi-Monthly)***"""

# list with semi monthly pay day range (15th and last day 30/31st of the month)

pay_day = pd.date_range(start=train_data.date.min(), end=train_data.date.max(), freq='SM').astype('str').tolist()

# create payday column

train_data["pay_day"] = np.where(train_data["date"].isin(pay_day), 1, 0)
train_data['pay_day'].value_counts()

"""We will get the average sales to see wether there is a difference in payday sales and other sales"""

# Plot the average sales in relation to payday

ax = train_data.groupby(["pay_day"], as_index=False)['sales'].mean()\
                                    .plot("pay_day", "sales", kind="bar", figsize=(12,6),
                                         title="Average sales in relation to payday")

for p in ax.patches:
    ax.annotate(format(p.get_height(), '.2f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                xytext = (0, 9),
                textcoords = 'offset points')

"""Average payday sales are slightly higher than non-payday sales

***Sales by day of the month***
"""

# plot the sum of sales by day of the month
# which day of the month has the most sales?

ax = train_data.groupby(["day_of_month"], as_index=False)["sales"].sum()\
                                    .plot("day_of_month", "sales", kind="bar", figsize=(12,6),
                                         title="Total sales by day of month")

"""Beginning of the month has high sales (1st - 3rd), with a gradual decline afterwards, but there is an overall uniform trend.

***Sales by week of the year***
"""

# plot the sum of sales by the weeks of the year

sales_by_week = train_data.groupby(["week_of_year"], as_index=False)['sales'].sum()

ax = sales_by_week.plot("week_of_year", "sales", kind="bar", figsize=(12, 6),
                         title="Total sales over the weeks of the year")

"""First week and mid year (week 27) have the highest sales

***Sales by month***
"""

# group the sum of sales by month of the year
# which month has the highest sales?

purchase_month = train_data.groupby("Month", as_index=False)["sales"].sum().sort_values(by="sales", ascending=False)
purchase_month = purchase_month.reset_index(drop=True)
purchase_month

# plot the sum of sales by month of the year

ax = purchase_month.plot("Month", "sales", kind="bar", figsize=(12, 6),
                         title="Total sales over the months of the year")

"""July(Summer) has the highest sales, followed by march (probaly easter), and december

***Sales by Day of the week***
"""

# group the sum of sales by day of the week
# which day has the highest sales?

purchase_day = train_data.groupby("Day", as_index=False)["sales"].sum().sort_values(by="sales", ascending=False)
purchase_day = purchase_day.reset_index(drop=True)
purchase_day

# plot the sum of sales by day of the week

ax = purchase_day.plot("Day", "sales", kind="bar", figsize=(12, 6),
                         title="Total sales over the days of the week")

"""Sunday and saturday (weekends) have the highest sales

***Sales by season***
"""

# create column for rainy and dry season

train_data["season"] = np.where(train_data["month"].isin([6,7,8,9]), 1, 0)

# plot sum of sales by season of the year (Dry or Rainy)
sales_by_season = train_data.groupby(["season"], as_index=False)['sales'].sum()

ax = sales_by_season.plot("season", "sales", kind="bar", figsize=(12, 6),
                          title="Total sales over seasons in Ecuador (Dry=0, Rainy=1)")

"""Tropical countries like Ecuador have 2 seasons (Dry and rainy)
Dry season has higher sales compared to rainy season

***Sales by quarter***
"""

# plot the sum of sales by quarter of the year

sales_by_quarter = train_data.groupby(["quarter"], as_index=False)['sales'].sum()

ax = sales_by_quarter.plot("quarter", "sales", kind="bar", figsize=(12, 6),
                          title="Total sales by quarter")

"""**7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)**

MSE( Mean Squared Error) is a widely used metric for regression problems. It measures the average of the squared differences between predicted and actual values. MSE is sensitive to outliers because it squares the differences.

RMSE (Root Mean Squared Error) is the square root of the MSE. It's often used because it has the same unit of measurement as the target variable, which makes it more interpretable. RMSE treats all errors equally.

RMSLE (Root Mean Squared Logarithmic Error) is commonly used when you want to penalize underpredictions more than overpredictions and when the target variable has a wide range. It first takes the natural logarithm of the predicted and actual values and then computes the RMSE of those log values. RMSLE takes the logarithm of the values before computing the error, which can be useful when dealing with skewed data or when you want to give less weight to large errors for large values

MSE is greater because it is calculated as the average of the squared differences between predicted and actual values. When you square the differences between predicted and actual values, even small errors get larger because negative differences (i.e., underpredictions) become positive. Large errors, in particular, contribute significantly to the MSE

**Hypothesis testing**

**Null Hypothesis (H0):** "There is no significant relationship between store sales and promotions."

**Alternative Hypothesis (Ha):** "There is a significant relationship between store sales and churn promotions."
"""

# encode categorical features

train_copy['onpromotion_encoded'] = np.where(train_copy['onpromotion'] >= 1, 1, 0)
train_copy['onpromotion_encoded'].value_counts()

"""**T-test**

The t-test is a statistical hypothesis test that assesses whether the means of two groups are significantly different from each other (an independent two-sample t-test)
"""

# Extract the sales of products when they are on promotion and when they are not
sales_on_promotion = train_copy[train_copy['onpromotion_encoded'] == 1]['sales']
sales_not_on_promotion = train_copy[train_copy['onpromotion_encoded'] == 0]['sales']

# Calculate the sample sizes
n1 = len(sales_on_promotion)
n2 = len(sales_not_on_promotion)

# Calculate the degrees of freedom
degrees_of_freedom = n1 + n2 - 2  # Assuming equal variances, subtract 2 for two samples

# Set the significance level
alpha = 0.05

# Calculate the critical t-values for a two-tailed test
critical_t_value = t.ppf(1 - alpha / 2, degrees_of_freedom)

# Perform an independent two-sample t-test
t_stat, p_value = ttest_ind(sales_on_promotion, sales_not_on_promotion)

# Print the t-statistic, p-value, degrees of freedom, and critical t-values
print("t-statistic:", t_stat)
print("p-value:", p_value)
print("Degrees of Freedom:", degrees_of_freedom)
print(f"Critical t-value (α = {alpha/2}):", -critical_t_value)  # Left tail
print(f"Critical t-value (α = {alpha/2}):", critical_t_value)   # Right tail

# Check if the absolute t-statistic falls within the critical value range
if t_stat >= -critical_t_value and t_stat <= critical_t_value:
    print("\nThe absolute t-statistic falls within the critical value range.")
else:
    print("\nThe absolute t-statistic does not fall within the critical value range.")

"""There is a statistically significant difference in sales between products when they are on promotion and when they are not on promotion (p-value < 0.05) and the t-statistic does not fall within the critical value range
We have strong evidence to reject the null hypothesis!!

***Chi-Squared contingency test***

The chi-squared statistic measures the strength of the association or dependence between the two categorical variables
"""

# Create contingency table
contingency_table = pd.crosstab(train_copy['onpromotion_encoded'], train_copy['sales'])

# Perform chi-squared test
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Print results
print("Chi-squared statistic: ", chi2)
print("p-value: ", p)

"""This indicates a strong association between the onpromotion and sales.

# 3. Data Preparation

***Data Issues***

Deal with missing vales after merging, creating new features and adding missing dates
Rename a few columns to be more readable
Ensure dtypes are correct for each column
"""

# shape overview to doubleckeck

train.shape, test.shape

train.info()

"""Our test data is without the target variable, which is ok"""

# check for duplicates

train.duplicated().any(),
stores.duplicated().any(),
oil.duplicated().any(),
test.duplicated().any(),
holidays.duplicated().any(),
transactions.duplicated().any()

# check for missing values in train data
# we know our test data doesn't have any missing values

train.isna().sum()

"""we have some missing values, since we added missing dates"""

null_rows = train[train['id'].isnull()]
null_rows

train.tail()

# replace null values in id column

train.loc[train['date'] == '2013-12-25', 'id'] = 3000888
train.loc[train['date'] == '2014-12-25', 'id'] = 3000889
train.loc[train['date'] == '2015-12-25', 'id'] = 3000890
train.loc[train['date'] == '2016-12-25', 'id'] = 3000891

cols = ["store_nbr", "sales", "onpromotion"]

# Fill null values in numerical columns with 0
train[cols] = train[cols].fillna(0)

# Fill null values in family column
train['family'] = train['family'].fillna('none')

train.isna().sum()

# we had some missing values in the oil dataset

oil['dcoilwtico'].isna().sum()

#Filling missing values in oil data with the the value before that missing data

oil= oil.bfill()

# check again to confirm

oil['dcoilwtico'].isna().sum()

"""***Preparing Train Data***"""

# Merge train and stores datasets
df_train1 = pd.merge(train, stores, on="store_nbr")

# Merge df_train1 with oil_df and holidays_df
merged_train = pd.merge(df_train1, oil, on="date", how="left")
merged_train = pd.merge(merged_train, holidays, on="date", how="left")

merged_train.head()

merged_train.info()

# drop categorical columns not relevant to analysis

merged_train= merged_train.drop(columns=["state", "type_x", "city", "locale", "locale_name", "description", "transferred"])
merged_train.head()

# remane columnswith relevant names

merged_train.rename(columns = {"cluster":"store_cluster", "dcoilwtico":"oil_price", "type_y":"events"}, inplace=True)

# check unique values in events column

merged_train["events"].unique()

# Rename values in events column

merged_train["events"].replace(to_replace={"Bridge", "Event", "Additional"}, value="Holiday", inplace=True)
merged_train["events"].replace(to_replace={"Work Day", "Transfer"}, value="No holiday", inplace=True)

# check for null values

merged_train["events"].isna().sum()

# Fill missing values with 'no holiday'

merged_train["events"].fillna("No holiday", inplace=True)

# Replace 'holidays' with 1 and other events with 0

merged_train['events'] = merged_train['events'].replace('Holiday', 1).replace('No holiday', 0)

# check unique values

merged_train["events"].unique()

# reclassification of product family to reduce number of categorical variables to encode

merged_train["family"].replace(to_replace={"GROCERY I", "GROCERY II", "EGGS", "PRODUCE", "DAIRY", "BREAD/BAKERY", "DELI", "PREPARED FOODS"}, value="GROCERY", inplace=True)
merged_train["family"].replace(to_replace={"HOME AND KITCHEN I", "HOME AND KITCHEN II", "HOME APPLIANCES", "HARDWARE", "PLAYERS AND ELECTRONICS"}, value="HOME AND KITCHEN", inplace=True)
merged_train["family"].replace(to_replace={"MEATS", "POULTRY", "SEAFOOD"}, value="FROZEN FOODS", inplace=True)
merged_train["family"].replace(to_replace={"HOME CARE", "LAWN AND GARDEN", "CLEANING"}, value="HOME CARE AND GARDEN", inplace=True)
merged_train["family"].replace(to_replace={"BEAUTY", "BABY CARE", "LADIESWEAR", "LINGERIE", "PERSONAL CARE"}, value="BEAUTY AND FASHION", inplace=True)
merged_train["family"].replace(to_replace={"LIQUOR,WINE,BEER", "BEVERAGES"}, value="BEVERAGES AND LIQUOR", inplace=True)
merged_train["family"].replace(to_replace={"MAGAZINES", "BOOKS", "CELEBRATION"}, value="SCHOOL AND OFFICE SUPPLIES", inplace=True)

# check unique values

merged_train["family"].unique()

# Check for missing values

merged_train.isna().sum()

# filling missing values

merged_train["oil_price"].fillna(method="bfill", inplace=True)

# check cols and rows

merged_train.shape

merged_train.info()

"""***Feature Engineering***"""

# Create new features

merged_train["Year"] = merged_train['date'].dt.year
merged_train["Month"] = merged_train['date'].dt.month
merged_train['Day'] = merged_train['date'].dt.day
merged_train['quarter'] = merged_train['date'].dt.quarter
merged_train['week_of_year'] = merged_train['date'].dt.isocalendar().week
merged_train['day_of_week'] = merged_train['date'].dt.dayofweek
merged_train["is_weekend"] = np.where(merged_train['day_of_week'] > 4, 1, 0)

merged_train['week_of_year'] = merged_train['week_of_year'].astype(int)

merged_train.tail()

merged_train.info()

# Create the directory if it doesn't exist
# import os

# os.makedirs('Data Tables')

# Now save the DataFrame to CSV
merged_train.to_csv('Data Tables/merged_train_data.csv', index=False)

csv_filename = 'Data Tables/merged_train_data.csv'
zip_filename = 'Data Tables/merged_train_data.zip'

# Create a ZIP archive and add the CSV file to it
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    zipf.write(csv_filename)

"""***Resampling***"""

# resample data to monthly frequency and compute the mean of sales for each month
cols = ['date', 'sales']

merged_monthly_mean = merged_train[cols].set_index('date').resample('M').mean()
print(merged_monthly_mean.shape)
merged_monthly_mean.tail(3)

"""***Stationarity Test***

A time series has to have some statistical properties to be able to stationary:

**Constant mean:** There is no trend over time.
Constant variance, covariance: The scale of the data doesn’t change and the relationship between observations is consistent.

**No seasonality:** Statistical properties of seasonal data changes regularly. Therefore, there is a pattern of change in the mean.
"""

fig = px.line(merged_monthly_mean, x=merged_monthly_mean.index, y='sales', title='Mean of Monthly Sales')
fig.update_xaxes(rangeslider_visible=True)

fig.show()

"""Ho (Null Hypothesis): The time series data is non-stationary
H1 (alternate Hypothesis): The time series data is stationary

***KPSS Test***
"""

stats, p, lags, critical_values=kpss(merged_train['sales'], 'ct')

print(f'Test_statistics: {stats}')
print(f'p-value: {p}')
print(f'Critical values: {critical_values}')

if p < 0.05 :
    print('Series is not stationary')
else :
    print('Series is stationary')

"""***Decomposing***"""

# first-order differencing on the target variable
differenced_data = merged_monthly_mean - merged_monthly_mean.shift(1)

# Plot the original and differenced time series
plt.figure(figsize=(12, 6))
plt.subplot(211)
plt.plot(merged_monthly_mean, label='Monthly mean')
plt.legend(loc='upper left')
plt.title('Monthly mean')

plt.subplot(212)
plt.plot(differenced_data, label='Differenced')
plt.legend(loc='upper left')
plt.title('Differenced Data')

plt.tight_layout()
plt.show()

stats, p, lags, critical_values=kpss(merged_monthly_mean, 'ct')

print(f'Test_statistics: {stats}')
print(f'p-value: {p}')
print(f'Critical values: {critical_values}')

if p < 0.05 :
    print('Series is not stationary')
else :
    print('Series is stationary')

"""***Encoding***"""

# using one hot encoder to encode family column

cat = ["family"]

# Initialize the OneHotEncoder
cat_train_encoder = OneHotEncoder()

# Fit and transform the categorical features
encoded_train_features = cat_train_encoder.fit_transform(merged_train[cat])

# Convert the encoded features to a DataFrame
encoded_train_df = pd.DataFrame(encoded_train_features.toarray(), columns=cat_train_encoder.get_feature_names_out(merged_train[cat].columns))

encoded_train_df

# encoded categories

cat_train_encoder.categories_

# to numpy array

encoded_train_features.toarray()

# Concatenate the encoded columns with the original data

merged_train_encoded = pd.concat([merged_train.reset_index(drop=True), encoded_train_df], axis=1)

merged_train_encoded.head()

# drop original cat column

final_train = merged_train_encoded.drop(columns = ["family"])

final_train.tail()

final_train.info()

"""***Scaling***"""

final_train.columns

# define the columns to be scaled
cols_to_scale = ['store_nbr', 'sales', 'onpromotion', 'store_cluster', 'oil_price',
                  'Year', 'Month', 'Day', 'quarter', 'week_of_year',
                  'day_of_week', 'is_weekend']

# instance of standard scaler
scaler = StandardScaler()

# output the transformed data as a pandas DataFrame instead of NumPy array
scaler.set_output(transform="pandas")

# fit scaler to selected columns
scaler.fit(final_train[cols_to_scale])

"""In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
"""

final_train_2 = final_train.copy()

# Transform the selected columns and replace the original columns with the scaled ones
final_train_2[cols_to_scale] = scaler.transform(final_train[cols_to_scale])

final_train_2.tail()

# set date column as index

final_train_2=final_train_2.set_index(['date'])

final_train_2.drop(columns='id', inplace=True)

final_train_2.tail()

# save to csv

final_train_2.to_csv('Data Tables/encoded_train_data.csv', index=False)

csv_filename = 'Data Tables/encoded_train_data.csv'
zip_filename = 'Data Tables/encoded_train_data.zip'

# Create a ZIP archive and add the CSV file to it
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    zipf.write(csv_filename)

"""***Train and Test Data***"""

merged_monthly_mean.shape

# splitting train and eval set for daily data

train_monthly = merged_monthly_mean[0:40]
eval_monthly = merged_monthly_mean[40:]

train_monthly.shape, eval_monthly.shape

"""# 4. Modelling

***1. AR model***
"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

acf = plot_acf(train_monthly['sales'], lags = 10)
pacf = plot_pacf(train_monthly['sales'], lags =10)

from statsmodels.tsa.ar_model import AutoReg

model_1 = AutoReg(train_monthly, lags = 6).fit()

model_1.summary()

AR_pred = model_1.predict (start = len(train_monthly),
                           end = len(train_monthly) + len(eval_monthly) - 1,
                           dynamic = False
                           )

plt.figure(figsize = (12,6))
plt.plot(train_monthly['sales'], label = 'Train')
plt.plot(eval_monthly['sales'], label = 'Eval')
plt.plot (AR_pred, label = 'AR Forecast')
plt.legend(loc='upper left')
plt.title('Auto Regressive Model')
plt.show()

from sklearn.metrics import mean_squared_error, mean_squared_log_error

mse = mean_squared_error(eval_monthly, AR_pred)
msle = mean_squared_log_error(eval_monthly, AR_pred)
rmse = np.sqrt(mean_squared_error(eval_monthly, AR_pred )).round(2)
rmsle = np.sqrt(mean_squared_log_error(eval_monthly, AR_pred)).round(2)

results_1 = pd.DataFrame([['AR', mse, msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])
results_1

"""***2. ARIMA mode***"""

# We use pmdarima to determine the best parameter for our ARIMA model.

!pip install pmdarima

from pmdarima import auto_arima

stepwise_fit = auto_arima(train_monthly['sales'], trace = True, suppress_warnings = True)
stepwise_fit.summary()

from statsmodels.tsa.arima.model import ARIMA

model_2 = ARIMA(train_monthly, order=(1,1,0))
model_2_fit = model_2.fit()
model_2_fit.summary()

# Make predictions on the test data
ARIMA_pred = model_2_fit.predict(start=len(train_monthly),
                                 end=len(train_monthly) + len(eval_monthly) - 1,
                                 typ="levels")

plt.figure(figsize = (12,6))
plt.plot(train_monthly['sales'], label = 'Train')
plt.plot(eval_monthly['sales'], label = 'Eval')
plt.plot (ARIMA_pred, label = 'ARIMA Forecast')
plt.legend(loc='upper left')
plt.title('Auto Regressive Integrated Moving Average Model')
plt.show()

mse = mean_squared_error(eval_monthly, ARIMA_pred)
msle = mean_squared_log_error(eval_monthly, ARIMA_pred)
rmse = np.sqrt(mean_squared_error(eval_monthly, ARIMA_pred )).round(2)
rmsle = np.sqrt(mean_squared_log_error(eval_monthly, ARIMA_pred)).round(2)

model_results = pd.DataFrame([['ARIMA', mse, msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])
results_2 = pd.concat([results_1, model_results], ignore_index=True)
results_2

"""***3. SARIMA model***"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

model_3 = SARIMAX(train_monthly, order=(1,1,0), seasonal_order=(1,0,0,12))
model_3_fit = model_3.fit()
model_3_fit.summary()

# Make predictions on the test data
SARIMA_pred = model_3_fit.predict(start=len(train_monthly),
                                 end=len(train_monthly) + len(eval_monthly) - 1,
                                 typ="levels")

plt.figure(figsize = (12,6))
plt.plot(train_monthly['sales'], label = 'Train')
plt.plot(eval_monthly['sales'], label = 'Eval')
plt.plot (SARIMA_pred, label = 'SARIMA Forecast')
plt.legend(loc='upper left')
plt.title('Seasonal Auto Regressive Integrated Moving Average Model')
plt.show()

mse = mean_squared_error(eval_monthly, SARIMA_pred)
msle = mean_squared_log_error(eval_monthly, SARIMA_pred)
rmse = np.sqrt(mean_squared_error(eval_monthly, SARIMA_pred )).round(2)
rmsle = np.sqrt(mean_squared_log_error(eval_monthly, SARIMA_pred)).round(2)

model_results = pd.DataFrame([['SARIMA', mse, msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])
results_3 = pd.concat([results_2, model_results], ignore_index=True)
results_3

"""# Traditional ML Models"""

final_train.shape

final_train.reset_index(inplace=True)
final_train.head()

final_train.drop(columns='date', inplace= True)

# Split data into parts
x = final_train.drop(['sales'], axis = 1)
y = final_train['sales']

# Split data into Train Test
X_train, X_test, y_train, y_test = x[0:2000000], x[2000000:], y[0:2000000], y[2000000:]

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""***4. Linear Regression***"""

from sklearn.linear_model import LinearRegression
lin_model = LinearRegression()
model_lin = lin_model.fit(X_train, y_train)

# Make prediction on X_test
lin_pred = model_lin.predict(X_test)

plt.figure(figsize=(8,4))
plt.plot(y_test, label ='Actual Sales')
plt.plot(lin_pred, label='Linear Regression')
plt.legend(loc='best')
plt.title('Linear Regression Prediction')
plt.show()

mse = mean_squared_error(y_test, lin_pred )
rmse = np.sqrt(mean_squared_error(y_test, lin_pred )).round(2)

results = pd.DataFrame([['Linear', mse, rmse]], columns = ['Model', 'MSE', 'RMSE'])
results_4 = pd.concat([results_3, results], ignore_index=True)
results_4

"""***5. Decision tree***"""

from sklearn.tree import DecisionTreeRegressor

decision_tree = DecisionTreeRegressor(random_state=42)
decision_tree_model = decision_tree.fit(X_train, y_train)

# Make prediction on test

predict_tree = decision_tree.predict(X_test)

#feature importance

plt.figure(figsize = (6,6))
plt.barh(X_train.columns, decision_tree_model.feature_importances_)

# predicted and actual data

plt.figure(figsize=(6,6))
plt.plot(y_test, label = "Actual Sales")
plt.plot(predict_tree, label = "Decision Tree")
plt.legend(loc = "best")
plt.title("Decision Tree Prediction")
plt.show()

# Evaluation metrics

mse = mean_squared_error(y_test, predict_tree)
msle = mean_squared_log_error(y_test, predict_tree)
rmse = np.sqrt(mean_squared_error(y_test, predict_tree )).round(2)
rmsle = np.sqrt(mean_squared_log_error(y_test, predict_tree)).round(2)

results = pd.DataFrame([['Decision Tree', mse,msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])
results_5 = pd.concat([results_4, results], ignore_index=True)
results_5

"""***6. Xg Boost***"""

from sklearn.ensemble import GradientBoostingRegressor

xgc = GradientBoostingRegressor(random_state=42)
xg_boost = xgc.fit(X_train, y_train)

# Predicting the Test set results
xg_boost_pred  = xgc.predict(X_test)

#feature importance

plt.figure(figsize = (6,6))
plt.barh(X_train.columns, xg_boost.feature_importances_)

# predicted and actual data

plt.figure(figsize=(12,8))
plt.plot(y_test, label = "Actual Sales")
plt.plot(xg_boost_pred, label = "xg_boostPrediction")
plt.legend(loc = "best")
plt.title("Gradient Boosting Regressor Prediction")
plt.show()

# Evaluation Metrics

mse = mean_squared_error(y_test, xg_boost_pred)
rmse = np.sqrt(mean_squared_error(y_test, xg_boost_pred)).round(2)

results = pd.DataFrame([['XG Boost', mse, rmse]], columns = ['Model', 'MSE', 'RMSE'])
results_6 = pd.concat([results_5, results], ignore_index=True)
results_6

"""***7. Random Forest***"""

# from sklearn.ensemble import RandomForestRegressor

# rf_model = RandomForestRegressor(random_state=42)
# rf_model.fit(X_train, y_train)

# # Make predictions
# rf_pred = rf_model.predict(X_test)

# # Feature Importance
# plt.figure(figsize=(6, 6))
# plt.barh(X_train.columns, rf_model.feature_importances_)
# # plt.title("Random Forest Feature Importance")
# # plt.xlabel("Feature Importance")
# # plt.ylabel("Features")
# # plt.show()

# # Plot predicted vs actual
# plt.figure(figsize=(12, 8))
# plt.plot(y_test, label="Actual Sales")
# plt.plot(rf_pred, label="Random Forest Prediction")
# plt.legend(loc="best")
# plt.title("Random Forest Prediction")
# plt.show()

# # Evaluation metrics
# from sklearn.metrics import mean_squared_error, mean_squared_log_error
# mse = mean_squared_error(y_test, rf_pred)
# msle = mean_squared_log_error(y_test, rf_pred)
# rmse = np.sqrt(mean_squared_error(y_test, rf_pred)).round(2)
# rmsle = np.sqrt(mean_squared_log_error(y_test, rf_pred)).round(2)

# results = pd.DataFrame([['Random Forest', mse, msle, rmse, rmsle]],
#                        columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])
# results_6 = pd.concat([results_5, results], ignore_index=True)
# results_6

merged_monthly_mean

backtests = {
    1: ('2017-05-31', '2017-06-30'),
    2: ('2017-06-30', '2017-07-31'),
    3: ('2017-07-31', '2017-08-31')
}

# Backtests with ARIMA Model
scores = {}

for period in backtests.values():

    # Convert date strings to datetime objects
    start_date = pd.to_datetime(period[0])
    end_date = pd.to_datetime(period[1])

    # Filter the DataFrame based on datetime comparisons
    _train = merged_monthly_mean.reset_index()[merged_monthly_mean.reset_index()['date'] < start_date]
    _test = merged_monthly_mean.reset_index()[(merged_monthly_mean.reset_index()['date'] >= start_date) & (merged_monthly_mean.reset_index()['date'] <= end_date)]

    Xtrain = _train.sales.values
    Xtest = _test.sales.values

    try:
        model = ARIMA(Xtrain, order=(1, 1, 0)).fit()

        # Initialize an empty array to store predictions
        ypred = []

        # Iterate through each data point in the test set and predict one step ahead
        for i in range(len(Xtest)):
            # Predict one step ahead by retraining the model
            ARIMA_model = ARIMA(Xtrain, order=(1, 1, 0)).fit()
            prediction = ARIMA_model.forecast(steps=1)[0]

            # Append the prediction to the result
            ypred.append(prediction)

            # Update the training data with the actual value
            Xtrain = np.append(Xtrain, Xtest[i])

        # Calculate RMSLE for the entire period
        rmsle = np.sqrt(mean_squared_log_error(Xtest, ypred))
        scores[str(period)] = rmsle
    except Exception as e:
        print(f"Error in period {period}: {str(e)}")

print(scores)

"""***Predictions***"""

merged_monthly_mean.index = pd.to_datetime(merged_monthly_mean.index)

merged_monthly_mean.index.max()

# Define the last observed date
last_observed_date = pd.to_datetime("2017-08-31")

# Define the end date of the year
end_of_year = pd.to_datetime("2017-12-31")

# Create a list of future dates for prediction (monthly frequency)
future_dates = pd.date_range(start=last_observed_date + pd.DateOffset(months=1), end=end_of_year, freq='M')

# Make predictions for future dates
# Replace 'steps' with the number of future predictions you want
predictions = ARIMA_model.forecast(steps=len(future_dates))

# Create a DataFrame to store predictions and dates
future_predictions = pd.DataFrame({'date': future_dates, 'predicted_sales': predictions})

# Print or save the predictions for analysis
print(future_predictions)